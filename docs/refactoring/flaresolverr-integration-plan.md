# FlareSolverr é›†æˆæ–¹æ¡ˆï¼ˆCF çº§åˆ« 2 è§£ç›¾ï¼‰

> **æ—¥æœŸ**: 2026-02-15 | **é¢„ä¼°**: 1â€“2 å¤©

### æ‰§è¡Œå‰ç½®æ¡ä»¶

| # | æ¡ä»¶ | å½“å‰çŠ¶æ€ | é˜»å¡ï¼Ÿ |
|:-:|------|---------|:------:|
| 1 | `codex/scraper-v2-refactor-20260215` åˆ†æ”¯åˆå…¥ `main` | â³ å¾…åˆå¹¶ï¼ˆ`personal` remote å·²æ¨é€ï¼Œ`origin` 403 éœ€æƒé™ï¼‰ | **æ˜¯** |
| 2 | `main` ä¸‹å­˜åœ¨ `scraper_v1/http_client.py`ã€`cf_solver.py` ç­‰ v2 æ–‡ä»¶ | âŒ ä»…åœ¨å·¥ä½œæ ‘åˆ†æ”¯ä¸­ | **æ˜¯** |
| 3 | 82 æœåŠ¡å™¨ä»£ç åŒæ­¥åˆ°åˆå¹¶åçš„æœ€æ–° `main` | â³ éœ€åœ¨æ¡ä»¶ 1 å®Œæˆåæ‰§è¡Œ | **æ˜¯** |

> [!CAUTION]
> æœ¬æ–¹æ¡ˆæ‰€æœ‰ä»£ç ä¿®æ”¹å‡åŸºäº scraper v2 åˆ†æ”¯ï¼ˆ`codex/scraper-v2-refactor-20260215`ï¼‰ã€‚åœ¨è¯¥åˆ†æ”¯åˆå…¥ `main` ä¹‹å‰ï¼Œ**ä¸å¯ç›´æ¥åœ¨ `main` ä¸Šæ‰§è¡Œ**ã€‚ä¸‹æ–¹æ–‡ä»¶è·¯å¾„å‡ä½¿ç”¨ç›¸å¯¹è·¯å¾„æ ¼å¼ `scraper_v1/xxx.py`ï¼Œå¯¹åº”å·¥ä½œæ ‘ä¸­çš„ `manga_translator/server/scraper_v1/xxx.py`ã€‚

---

## ä¸€ã€èƒŒæ™¯ä¸å®æµ‹è¯æ®

2026-02-15 å¯¹ `https://www.toongod.org/webtoon/teacher-yunji/` å®æµ‹ï¼š

| å¼•æ“ | ç»“æœ | å«ä¹‰ |
|------|------|------|
| aiohttp åŸç”Ÿ | `Can not decode content-encoding: br` | è¿å“åº”éƒ½è§£æä¸äº† |
| curl_cffi (impersonate=chrome120) | **403 â†’ 7713 bytes `"Just a moment..."`** | TLS æŒ‡çº¹é€šè¿‡ï¼Œä½†è§¦å‘ JS 5 ç§’ç›¾ |
| cf_solver.solve() | `ClientResponseError: 403` å¼‚å¸¸ | **Bugï¼šæ‹¿ä¸åˆ° HTML å°±å´©äº†** |

**ç»“è®º**ï¼šçº§åˆ« 1ï¼ˆTLS ä¼ªè£…ï¼‰å¯¹æ­¤ç«™ä¸å¤Ÿï¼Œéœ€è¦çº§åˆ« 2ï¼ˆFlareSolverr æ‰§è¡Œ JSï¼‰æ¥æ‹¿åˆ° `cf_clearance` Cookieã€‚

---

## äºŒã€éœ€ä¿®å¤çš„ Bugï¼ˆå‰ç½®æ¡ä»¶ï¼‰

### Bugï¼š`http_client.fetch_html()` ä¸¢å¼ƒ 403 å“åº”ä½“

```python
# http_client.py:205-206 â€” å½“å‰è¡Œä¸º
status = int(response.status_code)
if status >= 400:
    raise self._client_error(url, status, response.headers)  # HTML è¢«ä¸¢å¼ƒ
```

`cf_solver` éœ€è¦æ‹¿åˆ° 403 å“åº”çš„ HTML å†…å®¹æ¥åˆ¤æ–­æ˜¯"CF æŒ‘æˆ˜"è¿˜æ˜¯"çœŸæ­£æ‹’ç»"ï¼Œä½†å½“å‰å®ç°ç›´æ¥æŠ›å¼‚å¸¸ã€‚

#### [MODIFY] `scraper_v1/http_client.py`

å‚æ•°ä¼ æ’­è·¯å¾„è®¾è®¡ï¼ˆ3 å±‚ï¼‰ï¼š

```
fetch_html(allow_error_body=False)          â† å…¬å¼€ APIï¼Œcf_solver ä¼  True
  â””â†’ _request_text(allow_error_body)        â† åˆ†å‘å±‚ï¼Œé€ä¼ å‚æ•°
       â”œâ†’ _request_text_curl_cffi(allow_error_body)   â† curl_cffi å®ç°
       â””â†’ _request_text_aiohttp(allow_error_body)     â† aiohttp å®ç°ï¼ˆå« raise_for_statusï¼‰
```

**1) `fetch_html` ç­¾åå˜æ›´ï¼š**

```diff
  async def fetch_html(
      self,
      url: str,
      *,
      cookies: dict[str, str] | None = None,
      user_agent: str | None = None,
      referer: str | None = None,
+     allow_error_body: bool = False,
  ) -> str:
```

**2) `_request_text` å†…éƒ¨åˆ†å‘ï¼ˆçº¦ line 169ï¼‰ï¼š**

```diff
- return await self._request_text_curl_cffi(method, url, ...)
+ return await self._request_text_curl_cffi(method, url, ..., allow_error_body=allow_error_body)
  ...
- return await self._request_text_aiohttp(method, url, ...)
+ return await self._request_text_aiohttp(method, url, ..., allow_error_body=allow_error_body)
```

**3) `_request_text_curl_cffi`ï¼ˆçº¦ line 201ï¼‰ï¼š**

```diff
  async def _request_text_curl_cffi(
-     self, method, url, *, data, cookies, headers, timeout_sec
+     self, method, url, *, data, cookies, headers, timeout_sec, allow_error_body=False
  ) -> str:
      ...
      status = int(response.status_code)
      text = response.text
      if status >= 400:
+         if allow_error_body:
+             return text
          raise self._client_error(url, status, response.headers)
      return text
```

**4) aiohttp è·¯å¾„ï¼ˆçº¦ line 185ï¼‰ï¼š**

```diff
  async with session.request(...) as response:
      text = await response.text()
-     response.raise_for_status()
+     if response.status >= 400:
+         if allow_error_body:
+             return text
+         raise self._client_error(url, response.status, response.headers)
      return text
```

> [!IMPORTANT]
> - `allow_error_body` **é»˜è®¤ `False`**ï¼šæ‰€æœ‰ç°æœ‰è°ƒç”¨ç‚¹è¡Œä¸ºä¸å˜
> - **ä»… `cf_solver.solve()` ä¼  `True`**ï¼Œå…¶ä»–åœ°æ–¹ç¦æ­¢ä½¿ç”¨
> - `fetch_binary()` å’Œ `download_to_file()` ä¸å—å½±å“ï¼ˆå®ƒä»¬æœ‰ç‹¬ç«‹çš„é”™è¯¯å¤„ç†ï¼‰

#### [MODIFY] `scraper_v1/cf_solver.py`

```diff
  async def solve(self, url, *, current_cookies, user_agent, referer=None):
-     html = await self.http_client.fetch_html(url, ...)
+     html = await self.http_client.fetch_html(url, ..., allow_error_body=True)
      if not looks_like_challenge(html):
          return SolveResult(cookies=current_cookies, html=html, level_used="http_client")
```

---

## ä¸‰ã€FlareSolverr é›†æˆè®¾è®¡

### 3.1 æ¶æ„

```mermaid
sequenceDiagram
    participant P as Provider
    participant S as cf_solver
    participant H as HttpClient
    participant F as FlareSolverr
    participant C as CookieStore
    
    P->>S: solve(url, cookies)
    S->>H: fetch_html(url, allow_error_body=True)
    H-->>S: "Just a moment..." (CF æŒ‘æˆ˜)
    S->>S: looks_like_challenge() â†’ true
    
    alt FlareSolverr å¯ç”¨
        S->>F: POST {cmd: "request.get", url}
        F->>F: å¯åŠ¨ headless Chromeï¼Œç­‰å¾… JS æ‰§è¡Œ
        F-->>S: {solution: {response, cookies: [cf_clearance, ...]}}
        S->>C: ç¼“å­˜ cf_clearance Cookie
        S-->>P: SolveResult(cookies, html, level="flaresolverr")
    else FlareSolverr ä¸å¯ç”¨
        S-->>P: raise CloudflareChallengeError
        Note over P: å‰ç«¯æ”¶åˆ° SCRAPER_AUTH_CHALLENGE<br>æç¤ºç”¨æˆ·æ‰‹åŠ¨æ³¨å…¥ Cookieï¼ˆçº§åˆ« 4ï¼‰
    end
    
    Note over P,C: åç»­è¯·æ±‚è‡ªåŠ¨å¸¦ä¸Š cf_clearance<br>ç›´æ¥èµ° HttpClient æ— éœ€å†è¿‡ç›¾
```

### 3.2 FlareSolverr éƒ¨ç½²ï¼ˆDocker sidecarï¼‰

#### [NEW] `deploy/flaresolverr/docker-compose.yml`

```yaml
version: "3.8"
services:
  flaresolverr:
    image: ghcr.io/flaresolverr/flaresolverr:v3.4.6  # å›ºå®šç‰ˆæœ¬ï¼Œé¿å… latest è¡Œä¸ºæ¼‚ç§»
    container_name: flaresolverr
    restart: unless-stopped
    environment:
      - LOG_LEVEL=info
      - TZ=Asia/Shanghai
      - CAPTCHA_SOLVER=none       # ä¸ä½¿ç”¨ä»˜è´¹éªŒè¯ç æœåŠ¡
      - HEADLESS=true
    ports:
      - "127.0.0.1:8191:8191"     # ä»…ç»‘å®š localhostï¼Œä¸æš´éœ²å…¬ç½‘
    mem_limit: 512m               # é™åˆ¶å†…å­˜é¿å… Chrome è†¨èƒ€
    # æ³¨ï¼šdeploy.resources ä»…åœ¨ Docker Swarm ä¸‹ç”Ÿæ•ˆï¼Œæ™®é€š docker compose é  mem_limit é™åˆ¶å†…å­˜
    deploy:
      resources:
        limits:
          memory: 512M
```

å¯åŠ¨æ–¹å¼ï¼š

```bash
cd deploy/flaresolverr && docker compose up -d
```

#### åç«¯é…ç½®

ä»…éœ€è®¾ç½®ä¸€ä¸ªç¯å¢ƒå˜é‡ï¼š

```bash
# systemd drop-in æˆ– shell ç¯å¢ƒ
FLARESOLVERR_URL=http://localhost:8191/v1
```

> [!NOTE]
> ä¸è®¾ç½®æ­¤å˜é‡æ—¶ï¼Œcf_solver è‡ªåŠ¨è·³è¿‡ FlareSolverrï¼Œé€€åŒ–ä¸ºçº§åˆ« 4ï¼ˆäººåœ¨å›è·¯ï¼‰ã€‚**é›¶ä¾µå…¥æ€§**ã€‚

### 3.3 cf_solver å¢å¼º

#### [MODIFY] `scraper_v1/cf_solver.py`

åœ¨å½“å‰ 84 è¡Œéª¨æ¶åŸºç¡€ä¸Šå¢å¼ºä»¥ä¸‹èƒ½åŠ›ï¼š

**A. Cookie ç¼“å­˜é›†æˆ**

```python
class CloudflareSolver:
    def __init__(self, http_client, cookie_store: CookieStore | None = None):
        ...
        self._cookie_store = cookie_store

    async def solve(self, url, *, current_cookies, user_agent, referer=None):
        # 1. å…ˆæ£€æŸ¥ç¼“å­˜æ˜¯å¦æœ‰æœ‰æ•ˆçš„ cf_clearance
        domain = urlparse(url).hostname or ""
        if self._cookie_store:
            cached = self._cookie_store.get_cookies(domain)
            if "cf_clearance" in cached:
                merged = {**current_cookies, **cached}
                html = await self.http_client.fetch_html(
                    url, cookies=merged, user_agent=user_agent,
                    allow_error_body=True,
                )
                if not looks_like_challenge(html):
                    return SolveResult(cookies=merged, html=html, level_used="cached")

        # 2. curl_cffi ç›´è¿å°è¯•
        html = await self.http_client.fetch_html(
            url, cookies=current_cookies, user_agent=user_agent,
            allow_error_body=True,
        )
        if not looks_like_challenge(html):
            return SolveResult(cookies=current_cookies, html=html, level_used="http_client")

        # 3. FlareSolverr è§£ç›¾
        if self.flaresolverr_url:
            solved = await self._solve_with_flaresolverr(url=url, user_agent=user_agent)
            if solved is not None:
                # ç¼“å­˜æ‹¿åˆ°çš„ cf_clearance
                if self._cookie_store and solved.cookies:
                    self._cookie_store.update_cookies(domain, solved.cookies)
                return solved

        # 4. å…¨éƒ¨å¤±è´¥ â†’ æŠ›å¼‚å¸¸è§¦å‘äººåœ¨å›è·¯
        raise CloudflareChallengeError(
            f"CF æŒ‘æˆ˜æ— æ³•è‡ªåŠ¨è§£å†³ï¼Œè¯·æ‰‹åŠ¨æ³¨å…¥ Cookie (domain={domain})"
        )
```

**B. FlareSolverr è¶…æ—¶ä¸é‡è¯•**

```python
async def _solve_with_flaresolverr(self, *, url, user_agent) -> SolveResult | None:
    for attempt in range(2):  # æœ€å¤šé‡è¯• 1 æ¬¡
        try:
            timeout = aiohttp.ClientTimeout(total=60)  # CF 5ç§’ç›¾å®é™…å¯èƒ½ 10-15s
            payload = {
                "cmd": "request.get",
                "url": url,
                "maxTimeout": 45000,    # FlareSolverr å†…éƒ¨è¶…æ—¶
                "userAgent": user_agent,
            }
            async with aiohttp.ClientSession(timeout=timeout) as session:
                async with session.post(self.flaresolverr_url, json=payload) as resp:
                    if resp.status >= 500 and attempt == 0:
                        await asyncio.sleep(2)
                        continue    # é‡è¯•
                    if resp.status >= 400:
                        return None
                    body = await resp.json()
            ...  # è§£æ solutionï¼ˆç°æœ‰é€»è¾‘ä¿ç•™ï¼‰
        except (asyncio.TimeoutError, aiohttp.ClientError):
            if attempt == 0:
                await asyncio.sleep(2)
                continue
            return None
```

### 3.4 è°ƒç”¨æ–¹é›†æˆç‚¹

å½“å‰ Provider å‡½æ•°ï¼ˆsearch/catalog/chapters/reader_imagesï¼‰åœ¨é‡åˆ° CF æ—¶ç›´æ¥æŠ› `CloudflareChallengeError`ã€‚éœ€è¦åœ¨ Provider è°ƒç”¨å‰åŠ ä¸€å±‚è‡ªåŠ¨è§£ç›¾ï¼š

#### [MODIFY] `routes/v1_scraper.py`

åœ¨è·¯ç”±å±‚ä¸ºæ¯ä¸ªè¯·æ±‚è‡ªåŠ¨å°è¯•è§£ç›¾ï¼š

```python
async def _fetch_with_cf_solve(provider_fn, ctx: ProviderContext, target_url: str, *args):
    """åŒ…è£… Provider å‡½æ•°ï¼Œé‡ CF æŒ‘æˆ˜è‡ªåŠ¨è§£ç›¾åé‡è¯•
    
    target_url: å½“å‰è¯·æ±‚çš„ç²¾ç¡® URLï¼ˆmanga/chapterï¼‰ï¼Œä¼˜å…ˆç”¨äº CF è§£ç›¾ï¼Œ
                æ¯” base_url çš„å‘½ä¸­ç‡æ›´é«˜ã€‚
    """
    try:
        return await provider_fn(ctx, *args)
    except CloudflareChallengeError:
        solver = _get_cf_solver()
        # ä¼˜å…ˆç”¨ç›®æ ‡ URL è§£ç›¾ï¼Œå›é€€åˆ° base_url
        result = await solver.solve(
            target_url or ctx.base_url,
            current_cookies=ctx.cookies,
            user_agent=ctx.user_agent,
        )
        # ç”¨è§£ç›¾åçš„ cookies é‡è¯•ä¸€æ¬¡
        new_ctx = ProviderContext(
            **{**ctx.__dict__, "cookies": {**ctx.cookies, **result.cookies}}
        )
        return await provider_fn(new_ctx, *args)
```

```diff
  # search endpoint â€” search æ²¡æœ‰ç²¾ç¡®ç›®æ ‡ URLï¼Œä¼  base_url ä½œä¸º target_url
- items = await provider.search(base_url, req.keyword, ...)
+ items = await _fetch_with_cf_solve(
+     _provider_search_compat, ctx, base_url, provider, req.keyword
+ )

  # catalog endpoint â€” ç›®æ ‡ URL ä¸º catalog_path
- items, has_more = await provider.catalog(base_url, page, orderby, catalog_path, ...)
+ items, has_more = await _fetch_with_cf_solve(
+     _provider_catalog_compat, ctx, target_url, provider, page, orderby, catalog_path
+ )
+
+ # chapters endpoint â€” æœ‰ç²¾ç¡® manga_url
- items = await provider.chapters(base_url, manga_url, ...)
+ items = await _fetch_with_cf_solve(
+     _provider_chapters_compat, ctx, manga_url, provider, manga_url
+ )
```

---

## å››ã€æ–‡ä»¶å˜åŠ¨æ€»è§ˆ

| æ–‡ä»¶ | æ“ä½œ | æ”¹åŠ¨é‡ |
|------|------|:------:|
| `scraper_v1/http_client.py` | MODIFY | ~15 è¡Œ |
| `scraper_v1/cf_solver.py` | MODIFYï¼ˆå¢å¼ºï¼‰ | ~60 è¡Œ |
| `routes/v1_scraper.py` | MODIFYï¼ˆåŠ  `_fetch_with_cf_solve`ï¼‰ | ~25 è¡Œ |
| `deploy/flaresolverr/docker-compose.yml` | NEW | ~18 è¡Œ |
| `deploy/systemd/manga-translator.service` æˆ– drop-in conf | MODIFY | +2 è¡Œï¼ˆ`Environment=`ï¼‰ |

---

## äº”ã€éªŒè¯è®¡åˆ’

### è‡ªåŠ¨åŒ–æµ‹è¯•

```bash
# ç°æœ‰é—¨ç¦ä¸èƒ½å›å½’
cd /Users/xa/Desktop/projiect/worktrees/manga-translator-ui_scraper-v2-20260215
python -m pytest tests/test_v1_scraper_phase2.py tests/test_v1_scraper_phase3.py tests/test_v1_scraper_phase4.py -v
```

### æ–°å¢å•å…ƒæµ‹è¯•

| æµ‹è¯•ç”¨ä¾‹ | è¦†ç›–ç‚¹ |
|---------|--------|
| `test_fetch_html_allow_error_body_returns_html_on_403` | http_client ä¸æŠ›å¼‚å¸¸ï¼Œè¿”å› HTML |
| `test_fetch_html_default_raises_on_403` | é»˜è®¤è¡Œä¸ºä¸å˜ |
| `test_cf_solver_detects_challenge_and_falls_back` | cf_solver è¯†åˆ« CF æŒ‘æˆ˜åè°ƒç”¨ FlareSolverr mock |
| `test_cf_solver_caches_cookies_after_solve` | cf_clearance å†™å…¥ CookieStore |
| `test_cf_solver_no_flaresolverr_raises_challenge` | FlareSolverr ä¸å¯ç”¨æ—¶æ­£ç¡®æŠ›å‡ºå¼‚å¸¸ |

### åŠŸèƒ½å†’çƒŸæµ‹è¯• â€” éƒ¨ç½²åˆ° 82 æœåŠ¡å™¨

> **ç›®æ ‡æœåŠ¡å™¨**: `82.22.36.81` (root) | **ç°æœ‰éƒ¨ç½²**: systemdï¼ˆ`manga-translator.service`ï¼‰+ nginxï¼ˆ`manga-translator-82.conf`ï¼‰

#### æ­¥éª¤ 1ï¼šåŒæ­¥ä»£ç åˆ° 82 æœåŠ¡å™¨

> [!WARNING]
> å¿…é¡»åœ¨ scraper-v2 åˆ†æ”¯åˆå…¥ `main` åå†æ‰§è¡Œï¼Œå¦åˆ™æœåŠ¡å™¨ä¸Šç¼ºå°‘ v2 æ–‡ä»¶ã€‚

```bash
# ä»å·¥ä½œæ ‘åŒæ­¥åˆ°æœåŠ¡å™¨
rsync -avz --exclude='node_modules' --exclude='.git' --exclude='__pycache__' \
  /Users/xa/Desktop/projiect/worktrees/manga-translator-ui_scraper-v2-20260215/ \
  root@82.22.36.81:/root/manhua-translator/
```

#### æ­¥éª¤ 2ï¼šåœ¨ 82 æœåŠ¡å™¨ä¸Šéƒ¨ç½² FlareSolverr

```bash
ssh root@82.22.36.81 << 'EOF'
# æ£€æŸ¥ Docker æ˜¯å¦å¯ç”¨
docker --version

# åˆ›å»º docker-compose æ–‡ä»¶
mkdir -p /root/manhua-translator/deploy/flaresolverr
cat > /root/manhua-translator/deploy/flaresolverr/docker-compose.yml << 'YAML'
version: "3.8"
services:
  flaresolverr:
    image: ghcr.io/flaresolverr/flaresolverr:v3.4.6
    container_name: flaresolverr
    restart: unless-stopped
    environment:
      - LOG_LEVEL=info
      - TZ=Asia/Shanghai
      - CAPTCHA_SOLVER=none
      - HEADLESS=true
    ports:
      - "127.0.0.1:8191:8191"   # ä»…ç»‘å®š localhostï¼Œä¸æš´éœ²å…¬ç½‘
    mem_limit: 512m
    deploy:
      resources:
        limits:
          memory: 512M
YAML

# æ‹‰å–å¹¶å¯åŠ¨
cd /root/manhua-translator/deploy/flaresolverr
docker compose up -d

# ç­‰å¾… Chrome å¯åŠ¨ï¼ˆçº¦ 15sï¼‰
sleep 15
curl -s http://localhost:8191 | head -1
# æœŸæœ›ï¼šè¿”å› FlareSolverr ç‰ˆæœ¬ä¿¡æ¯ JSON
EOF
```

#### æ­¥éª¤ 3ï¼šé…ç½®åç«¯ç¯å¢ƒå˜é‡

```bash
ssh root@82.22.36.81 << 'EOF'
# åœ¨ systemd æœåŠ¡ä¸­æ³¨å…¥ç¯å¢ƒå˜é‡
mkdir -p /etc/systemd/system/manga-translator.service.d
cat > /etc/systemd/system/manga-translator.service.d/flaresolverr.conf << 'CONF'
[Service]
Environment="FLARESOLVERR_URL=http://localhost:8191/v1"
Environment="SCRAPER_HTTP_ENGINE=curl_cffi"
CONF

# å®‰è£… curl_cffiï¼ˆå¦‚æœªå®‰è£…ï¼‰
pip install curl_cffi

# é‡å¯æœåŠ¡ï¼ˆå¯åŠ¨å‘½ä»¤ä¸º python -m manga_translator webï¼‰
systemctl daemon-reload
systemctl restart manga-translator
sleep 5
systemctl status manga-translator --no-pager
EOF
```

#### æ­¥éª¤ 4ï¼šè¿œç¨‹åŠŸèƒ½éªŒè¯

> **æ¥å£å¥‘çº¦**ï¼šè®¤è¯ä½¿ç”¨ `X-Session-Token` å¤´æˆ– `?token=` æŸ¥è¯¢å‚æ•°ï¼ˆè§ `middleware.py:97`ï¼‰ï¼›
> `/chapters` éœ€è¦ `manga` å¯¹è±¡å« `id`/`title`ï¼ˆè§ `routes/v1_scraper.py` çš„ `ScraperChaptersRequest`ï¼‰ï¼›systemd ç«¯å£ä¸º `8000`ã€‚

```bash
SSH="ssh root@82.22.36.81"

# é€šè¿‡ POST /auth/login è·å–ä¼šè¯ä»¤ç‰Œ
# LoginRequest éœ€è¦ username + passwordï¼ˆè§ auth.py:33-36ï¼‰
# LoginResponse è¿”å› {success, token, user, ...}ï¼ˆè§ auth.py:57-63ï¼‰
TOKEN=$($SSH 'curl -s -X POST http://localhost:8000/auth/login \
  -H "Content-Type: application/json" \
  -d "{\"username\":\"admin\",\"password\":\"YOUR_PASSWORD\"}" \
  | python3 -c "import sys,json; r=json.load(sys.stdin); print(r.get(\"token\",\"\") if r.get(\"success\") else \"LOGIN_FAILED: \"+r.get(\"message\",\"\"))"')
echo "Session Token: ${TOKEN:0:8}..."
# æ³¨æ„ï¼šè¯·å°† admin / YOUR_PASSWORD æ›¿æ¢ä¸ºå®é™…çš„ç”¨æˆ·åå’Œå¯†ç 

# Test A: FlareSolverr å¥åº·æ£€æŸ¥
$SSH 'curl -s http://localhost:8191'
# æœŸæœ›ï¼šè¿”å›ç‰ˆæœ¬ JSON

# Test B: ç›´æ¥æµ‹è¯• CF è§£ç›¾ï¼ˆç»•è¿‡ Web å‰ç«¯ï¼‰
$SSH 'curl -s -X POST http://localhost:8191/v1 \
  -H "Content-Type: application/json" \
  -d "{\"cmd\":\"request.get\",\"url\":\"https://www.toongod.org/webtoon/teacher-yunji/\",\"maxTimeout\":45000}" \
  | python3 -c "import sys,json; d=json.load(sys.stdin); print(\"status:\",d.get(\"status\")); s=d.get(\"solution\",{}); print(\"response_len:\",len(s.get(\"response\",\"\"))); print(\"cookies:\", [c[\"name\"] for c in s.get(\"cookies\",[])])"'
# æœŸæœ›ï¼šstatus: ok, response_len è¾ƒå¤§, cookies ä¸­åŒ…å« cf_clearance

# Test C: é€šè¿‡åç«¯ API æµ‹è¯•å®Œæ•´é“¾è·¯ï¼ˆç«¯å£ 8000ï¼ŒX-Session-Token è®¤è¯ï¼Œmanga å¯¹è±¡ï¼‰
$SSH "curl -s -X POST http://localhost:8000/api/v1/scraper/chapters \
  -H 'Content-Type: application/json' \
  -H 'X-Session-Token: ${TOKEN}' \
  -d '{\"base_url\":\"https://www.toongod.org\",\"manga\":{\"id\":\"teacher-yunji\",\"title\":\"Teacher Yunji\"},\"http_mode\":true}' \
  | python3 -c 'import sys,json; d=json.load(sys.stdin); print(\"chapters:\",len(d) if isinstance(d,list) else d)'"
# æœŸæœ›ï¼šè¿”å›ç« èŠ‚åˆ—è¡¨æ•°ç»„

# Test D: Cookie ç¼“å­˜éªŒè¯ï¼ˆç¬¬äºŒæ¬¡åº”æ›´å¿«ï¼Œå‘½ä¸­ cf_clearance ç¼“å­˜ï¼‰
$SSH "time curl -s -X POST http://localhost:8000/api/v1/scraper/chapters \
  -H 'Content-Type: application/json' \
  -H 'X-Session-Token: ${TOKEN}' \
  -d '{\"base_url\":\"https://www.toongod.org\",\"manga\":{\"id\":\"teacher-yunji\",\"title\":\"Teacher Yunji\"},\"http_mode\":true}' \
  | python3 -c 'import sys,json; d=json.load(sys.stdin); print(\"chapters:\",len(d) if isinstance(d,list) else d)'"
# æœŸæœ›ï¼šæ›´å¿«å“åº”

# Test E: FlareSolverr å®•æœºå›é€€
$SSH "docker stop flaresolverr && curl -s -X POST http://localhost:8000/api/v1/scraper/chapters \
  -H 'Content-Type: application/json' \
  -H 'X-Session-Token: ${TOKEN}' \
  -d '{\"base_url\":\"https://www.toongod.org\",\"manga\":{\"id\":\"teacher-yunji\",\"title\":\"Teacher Yunji\"},\"http_mode\":true}' \
  | python3 -m json.tool"
# æœŸæœ›ï¼šè¿”å›é”™è¯¯ç  SCRAPER_AUTH_CHALLENGE

# æ¢å¤ FlareSolverr
$SSH 'docker start flaresolverr'
```

#### æ­¥éª¤ 5ï¼šå‰ç«¯é›†æˆéªŒè¯ï¼ˆæµè§ˆå™¨ï¼‰

1. æ‰“å¼€ `http://82.22.36.81/` â†’ è¿›å…¥ Scraper é¡µé¢
2. è¾“å…¥ `https://www.toongod.org` ä½œä¸ºç«™ç‚¹ URL
3. æœç´¢ "teacher yunji" â†’ **æœŸæœ›**ï¼šè¿”å›æœç´¢ç»“æœ
4. ç‚¹å‡»è¿›å…¥ â†’ æŸ¥çœ‹ç« èŠ‚åˆ—è¡¨ â†’ **æœŸæœ›**ï¼šæ˜¾ç¤ºç« èŠ‚åˆ—è¡¨
5. é€‰æ‹©ä¸€ç« ä¸‹è½½ â†’ **æœŸæœ›**ï¼šä¸‹è½½æ­£å¸¸è¿›è¡Œ

---

## å…­ã€é£é™©

| é£é™© | çº§åˆ« | ç¼“è§£ |
|------|:----:|------|
| FlareSolverr è§£ç›¾è¶…æ—¶ï¼ˆCF 5 ç§’ç›¾å®é™…å¯èƒ½ 15s+ï¼‰ | ğŸŸ¡ ä¸­ | è¶…æ—¶è®¾ 60sï¼Œå†…éƒ¨ maxTimeout 45s |
| FlareSolverr å†…å­˜è†¨èƒ€ï¼ˆé•¿æœŸè¿è¡Œ Chromeï¼‰ | ğŸŸ¡ ä¸­ | `mem_limit: 512m` + `restart: unless-stopped` |
| cf_clearance Cookie è¿‡æœŸåéœ€å†æ¬¡è¿‡ç›¾ | ğŸŸ¢ ä½ | CookieStore æ£€æŸ¥ expiresï¼Œè¿‡æœŸè‡ªåŠ¨é‡æ–° solve |
| `allow_error_body` è¢«å¤–éƒ¨è¯¯ç”¨ | ğŸŸ¢ ä½ | å‚æ•°å‘½åæ¸…æ™° + æ–‡æ¡£æ ‡æ³¨ä»…ä¾›å†…éƒ¨ä½¿ç”¨ |
| 82 æœåŠ¡å™¨ Docker é•œåƒæ‹‰å–æ…¢ | ğŸŸ¡ ä¸­ | FlareSolverr é•œåƒçº¦ 500MBï¼Œé¦–æ¬¡æ‹‰å–å¯èƒ½éœ€ 5-10 åˆ†é’Ÿ |
| äº‘æœåŠ¡å™¨ IP åœ¨ CF é»‘åå•ä¸­ | ğŸ”´ é«˜ | FlareSolverr å¯èƒ½ä¹Ÿè¢« CF è¯†åˆ«ä¸ºæ•°æ®ä¸­å¿ƒ IPï¼Œè§£ç›¾å¤±è´¥ç‡æ›´é«˜ï¼›é€€åŒ–åˆ°çº§åˆ« 4ï¼ˆäººå·¥æ³¨å…¥ï¼‰ |
| `curl_cffi` åœ¨æœåŠ¡å™¨ Linux ä¸Šå®‰è£…å¤±è´¥ | ğŸŸ¡ ä¸­ | æ¨èä½¿ç”¨ `python:3.12-slim`ï¼ˆglibcï¼‰ï¼Œpip æä¾›é¢„ç¼–è¯‘ wheel |
