# çˆ¬è™«æ¨¡å— v2 é‡æ„æ–¹æ¡ˆ

> **æ—¥æœŸ**: 2026-02-15 | **çŠ¶æ€**: æ‰§è¡Œä¸­ï¼ˆFlareSolverr é›†æˆå·²è½åœ°åˆ° v2 åˆ†æ”¯ï¼‰ | **é¢„ä¼°æ€»å·¥æ—¶**: çº¦ 2 å‘¨ï¼ˆå•äººï¼‰

---

## é—®é¢˜æ‘˜è¦

å½“å‰çˆ¬è™«æ¨¡å—ï¼ˆ`scraper_v1`ï¼‰åŠŸèƒ½å®Œæ•´ï¼Œä½†å­˜åœ¨ä»¥ä¸‹æ ¸å¿ƒæ¶æ„é—®é¢˜ï¼š

1. **è·¯ç”±å±‚è†¨èƒ€**: `v1_scraper.py` è¾¾ 1260 è¡Œï¼Œæ··åˆæ¨¡å‹/ä¸šåŠ¡/çŠ¶æ€/è·¯ç”±å››é‡èŒè´£
2. **ç½‘ç»œè¯·æ±‚æ•£è½**: `mangaforfree.py`ã€`generic.py`ã€`v1_scraper.py` å„è‡ªåˆ›å»º aiohttp session
3. **å†…å­˜/SQLite åŒå†™ä¸ä¸€è‡´**: ä»»åŠ¡**åˆ›å»º**æ—¶ SQLite å¤±è´¥ä¼šæ­£ç¡®æŠ›å‡º `SCRAPER_TASK_STORE_ERROR`ï¼Œä½†ä»»åŠ¡**çŠ¶æ€æ›´æ–°**æ—¶ SQLite å¤±è´¥è¢«é™é»˜åæ‰ï¼ˆ`v1_scraper.py:560-562`ï¼‰ï¼ŒæœåŠ¡é‡å¯å¯èƒ½ä¸¢å¤±ä¸­é—´çŠ¶æ€
4. **ç§æœ‰å‡½æ•°è·¨æ¨¡å—è€¦åˆ**: `toongod.py` å¯¼å…¥ `mangaforfree.py` çš„ 14 ä¸ªç¬¦å·ï¼ˆå…¶ä¸­ 8 ä¸ªä¸º `_` å‰ç¼€ç§æœ‰å‡½æ•°ï¼Œ`toongod.py:10`ï¼‰
5. **Cloudflare å¯¹æŠ—èƒ½åŠ›å¼±**: è£¸ aiohttp çš„ TLS æŒ‡çº¹ææ˜“è¢«è¯†åˆ«

---

## æ€»ä½“æ¶æ„ï¼ˆé‡æ„åï¼‰

```mermaid
graph TD
    subgraph å‰ç«¯
        A[ScraperView.vue]
        B[scraper store<br>æ‹†åˆ†ä¸º composables]
    end

    subgraph è·¯ç”±å±‚
        C["v1_scraper.py<br>(ç˜¦è·¯ç”±, ~300L)"]
    end

    subgraph ä¸šåŠ¡æœåŠ¡å±‚
        D[download_service.py<br>ä¸‹è½½ç¼–æ’ + ä»»åŠ¡ç®¡ç†]
    end

    subgraph Provider æ’ä»¶å±‚
        E[base.py<br>BaseProvider + å…±äº«è§£æ]
        F[mangaforfree.py]
        G[toongod.py]
        H[generic.py]
    end

    subgraph åŸºç¡€è®¾æ–½å±‚
        I["http_client.py<br>ç»Ÿä¸€ HttpClient (curl_cffi)"]
        J[task_store.py<br>SQLite å•ä¸€æ•°æ®æº]
        K[state.py<br>Cookie å…¨å±€ç¼“å­˜]
        L[alerts.py<br>å‘Šè­¦å¼•æ“]
        M["cf_solver.py<br>Cloudflare é˜¶æ¢¯å¯¹æŠ—"]
    end

    A --> B --> C
    C --> D
    D --> E
    E --> F & G & H
    F & G & H --> I
    I --> M
    D --> J
    D --> K
    I --> K
    D --> L
```

---

## Phase 1: ç»Ÿä¸€ç½‘ç»œå±‚ + Cloudflare å¯¹æŠ—ï¼ˆ8hï¼‰

> è¿™æ˜¯æ”¶ç›Šæœ€å¤§ã€æœ€ä¼˜å…ˆçš„æ”¹åŠ¨ã€‚

### 1.1 æ–°å»º `scraper_v1/http_client.py` â€” ç»Ÿä¸€ HttpClient

**ç›®æ ‡**: æ‰€æœ‰ HTTP è¯·æ±‚æ”¶æ•›åˆ°ä¸€ä¸ªå…¥å£ï¼Œæ¶ˆé™¤æ•£è½çš„ `aiohttp.ClientSession` åˆ›å»ºã€‚

```python
class ScraperHttpClient:
    """å…¨å±€å”¯ä¸€çš„çˆ¬è™« HTTP å®¢æˆ·ç«¯"""

    def __init__(self, default_user_agent: str, cookie_store: CookieStore):
        self._cookie_store = cookie_store
        self._domain_semaphores: dict[str, asyncio.Semaphore] = {}
        self._default_ua = default_user_agent

    async def fetch_html(self, url, *, cookies=None, user_agent=None, referer=None) -> str:
        """ç»Ÿä¸€ HTML è·å–ï¼Œè‡ªåŠ¨æ³¨å…¥ cookies/referer/UA"""

    async def download_binary(self, url, output_path, *, referer, cookies=None) -> DownloadResult:
        """ç»Ÿä¸€äºŒè¿›åˆ¶ä¸‹è½½ï¼ˆå›¾ç‰‡ç­‰ï¼‰ï¼Œè‡ªåŠ¨é™æµ"""
```

**æ ¸å¿ƒèƒ½åŠ›**:

| èƒ½åŠ› | å®ç°æ–¹å¼ |
|------|---------|
| TLS æŒ‡çº¹ä¼ªè£… | åº•å±‚ä½¿ç”¨ `curl_cffi.requests.AsyncSession(impersonate="chrome120")`ï¼Œé€šè¿‡ feature flag ç°åº¦åˆ‡æ¢ |
| åŸŸåçº§å¹¶å‘é™æµ | æ¯ä¸ªåŸŸåç‹¬ç«‹ `asyncio.Semaphore`ï¼Œå¯é…ç½® |
| Cookie è‡ªåŠ¨æ³¨å…¥ | ä» `CookieStore` è¯»å–å½“å‰åŸŸåæœ‰æ•ˆ Cookie |
| Referer è‡ªåŠ¨å¡«å…… | ä¸‹è½½æ—¶å¼ºåˆ¶å¸¦ä¸Šæ¥æºé¡µé¢ URL |
| è¶…æ—¶ä¸é‡è¯• | å†…ç½®åˆ†çº§é‡è¯•ç­–ç•¥ |

**å½±å“èŒƒå›´**:

| æ–‡ä»¶ | æ”¹åŠ¨ |
|------|------|
| `mangaforfree.py` | åˆ é™¤ `_fetch_html()`ï¼Œæ”¹ä¸ºè°ƒç”¨ `HttpClient.fetch_html()` |
| `generic.py` | åˆ é™¤ `_fetch_html_http()` + `_fetch_html_playwright_sync()`ï¼Œæ”¹ä¸ºè°ƒç”¨ HttpClient |
| `toongod.py` | ä¸å†å¯¼å…¥ `mangaforfree._fetch_html`ï¼Œæ”¹ä¸ºä½¿ç”¨ HttpClient |
| `v1_scraper.py` | åˆ é™¤ `_download_image()` å‡½æ•°ï¼Œæ”¹ä¸ºè°ƒç”¨ `HttpClient.download_binary()` |

### 1.2 Feature Flag ç°åº¦ç­–ç•¥

> [!IMPORTANT]
> `curl_cffi` åˆ‡æ¢ä¸å¯ä¸€æ¬¡æ€§å…¨é‡æ›¿æ¢ã€‚é€šè¿‡ç¯å¢ƒå˜é‡æ§åˆ¶ç°åº¦ï¼š

```python
# http_client.py
USE_CURL_CFFI = os.environ.get("SCRAPER_HTTP_ENGINE", "aiohttp")  # "aiohttp" | "curl_cffi"
```

- **Phase 1 åˆæœŸ**ï¼šé»˜è®¤ `aiohttp`ï¼Œæ‰‹åŠ¨å¼€å¯ `curl_cffi` éªŒè¯
- **Phase 1 ç¨³å®šå**ï¼šé»˜è®¤ `curl_cffi`ï¼Œä¿ç•™ `aiohttp` å›é€€
- **æœ€ç»ˆ**ï¼šç§»é™¤ `aiohttp` ä»£ç è·¯å¾„

### 1.3 æ–°å»º `scraper_v1/cf_solver.py` â€” Cloudflare é˜¶æ¢¯å¯¹æŠ—

**ç­–ç•¥**: é‡åˆ° CF æ—¶è‡ªåŠ¨å‡çº§å¯¹æŠ—çº§åˆ«ï¼Œå¯¹è°ƒç”¨æ–¹é€æ˜ã€‚

```
çº§åˆ« 1: curl_cffi TLS ä¼ªè£…ï¼ˆé»˜è®¤ï¼Œé›¶é¢å¤–å¼€é”€ï¼‰
    â†“ è‹¥è¿”å› 503 challenge
çº§åˆ« 2: FlareSolverr æ—è·¯è§£ç›¾ï¼ˆå¯é€‰ sidecar å®¹å™¨ï¼‰
    â†“ è‹¥ FlareSolverr ä¸å¯ç”¨æˆ–å¤±è´¥
çº§åˆ« 3: æš‚åœä»»åŠ¡ + å‰ç«¯å‘Šè­¦ï¼Œè¯·æ±‚äººå·¥æ³¨å…¥ Cookie
```

```python
class CloudflareSolver:
    async def solve(self, url: str, current_cookies: dict) -> SolveResult:
        """
        è¿”å› SolveResult(cookies=..., html=..., level_used=...)
        è‡ªåŠ¨æŒ‰çº§åˆ«å°è¯•ï¼Œå¤±è´¥æ—¶è§¦å‘å‘Šè­¦
        """
```

**å…³é”®è®¾è®¡å†³ç­–**:

| å†³ç­– | é€‰æ‹© | ç†ç”± |
|------|------|------|
| FlareSolverr æ˜¯å¦å¿…é¡» | **å¯é€‰** | é€šè¿‡ç¯å¢ƒå˜é‡ `FLARESOLVERR_URL` æ§åˆ¶ï¼Œä¸é…ç½®åˆ™è·³è¿‡ |
| DrissionPage æ˜¯å¦é›†æˆ | **å»¶å** | ä¸é€‚åˆ Docker éƒ¨ç½²ï¼Œå…ˆä¸åš |
| Cookie æœ‰æ•ˆæœŸç®¡ç† | Cookie è·å–åç¼“å­˜åˆ° `CookieStore`ï¼Œè‡ªåŠ¨æ£€æµ‹è¿‡æœŸ | é¿å…æ¯æ¬¡è¯·æ±‚éƒ½è¿‡ç›¾ |

### 1.3 å¢å¼º `scraper_v1/state.py` â€” Cookie å…¨å±€ç¼“å­˜

åœ¨ç°æœ‰ `state.py` åŸºç¡€ä¸Šå¢åŠ **å†…å­˜ç¼“å­˜å±‚**ï¼š

```python
class CookieStore:
    """è·¨ä»»åŠ¡ã€è·¨è¯·æ±‚çš„ Cookie ç¼“å­˜ç®¡ç†"""

    def get_cookies(self, domain: str) -> dict[str, str]
    def update_cookies(self, domain: str, cookies: dict[str, str], expires_at: float | None)
    def invalidate(self, domain: str)
```

æ›¿ä»£ç°æœ‰ `_merge_cookies()` æ¯æ¬¡ä»æ–‡ä»¶ç³»ç»Ÿé‡æ–°è¯»å–çš„åšæ³•ã€‚

---

## Phase 2: Provider åŸºç±»é‡æ„ + å…±äº«é€»è¾‘æŠ½å–ï¼ˆ4hï¼‰

### 2.1 æ–°å»º `scraper_v1/base.py` â€” æŠ½å–å…±äº«è§£æé€»è¾‘

ä» `mangaforfree.py` æå–ä»¥ä¸‹ç§æœ‰å‡½æ•°åˆ°å…¬å…±æ¨¡å—ï¼š

| å‡½æ•° | ç”¨é€” | å½“å‰è¢«è°å¯¼å…¥ |
|------|------|-------------|
| `_infer_slug()` | URL â†’ slug | toongod, generic |
| `_normalize_url()` | ç›¸å¯¹â†’ç»å¯¹ URL | toongod, generic |
| `_canonical_series_url()` | è§„èŒƒåŒ–ç³»åˆ— URL | toongod, generic |
| `_looks_like_challenge()` | æ£€æµ‹ CF æŒ‘æˆ˜é¡µ | toongod, genericï¼ˆè¿ç§»åˆ° cf_solverï¼‰ |
| `_request_headers()` | æ„é€ è¯·æ±‚å¤´ | toongodï¼ˆè¿ç§»åˆ° HttpClientï¼‰ |
| `_fetch_html()` | HTTP è·å– HTML | toongodï¼ˆè¿ç§»åˆ° HttpClientï¼‰ |
| `_extract_ajax_config()` | WP AJAX é…ç½®æå– | toongod |
| `_fetch_chapters_via_ajax()` | AJAX è·å–ç« èŠ‚ | toongod |
| `parse_catalog_has_more()` | åˆ†é¡µæ£€æµ‹ | toongod, generic |

### 2.2 è§„èŒƒåŒ– Provider æ¥å£

ä¿ç•™ç°æœ‰ `ProviderAdapter` dataclass æ¨¡å¼ï¼ˆä¸è¿ç§»åˆ° ABCï¼‰ï¼Œä½†æ”¹å–„è°ƒç”¨æ–¹å¼ï¼š

```python
@dataclass
class ProviderContext:
    """ç»Ÿä¸€çš„è¯·æ±‚ä¸Šä¸‹æ–‡ï¼Œæ›¿ä»£ 6 ä¸ªä½ç½®å‚æ•°"""
    base_url: str
    cookies: dict[str, str]
    user_agent: str
    http_mode: bool
    force_engine: str | None

@dataclass(frozen=True)
class ProviderAdapter:
    key: str
    label: str
    hosts: tuple[str, ...]
    # ... å…ƒæ•°æ®å­—æ®µä¿ç•™ ...
    search: Callable[[ProviderContext, str], Awaitable[list[MangaItem]]]
    catalog: Callable[[ProviderContext, int, str | None, str | None], Awaitable[tuple[list[MangaItem], bool]]]
    chapters: Callable[[ProviderContext, str], Awaitable[list[ChapterItem]]]
    reader_images: Callable[[ProviderContext, str], Awaitable[list[str]]]
```

> [!NOTE]
> **ä¸ºä»€ä¹ˆä¸ç”¨ ABC?** ç°æœ‰ dataclass å‡½æ•°å¼ç»„åˆå·²ç»è¶³å¤Ÿçµæ´»ï¼Œä¸”é¿å…äº†ç»§æ‰¿å±‚æ¬¡å¤æ‚åŒ–ã€‚`ProviderContext` è§£å†³äº†ä½ç½®å‚æ•°å®¹æ˜“ä¼ é”™çš„æ ¸å¿ƒé—®é¢˜ã€‚

---

## Phase 3: è·¯ç”±å±‚ç˜¦èº« + ä»»åŠ¡ç®¡ç†é‡æ„ï¼ˆ8hï¼‰

### 3.1 æ‹†åˆ† `v1_scraper.py`

å°† 1260 è¡Œæ‹†ä¸º 4 ä¸ªæ–‡ä»¶ï¼š

| æ–°æ–‡ä»¶ | èŒè´£ | é¢„ä¼°è¡Œæ•° |
|--------|------|:--------:|
| `routes/v1_scraper.py` | çº¯è·¯ç”±å¤„ç†ï¼ˆå‚æ•°æ ¡éªŒ + è°ƒç”¨æœåŠ¡ + è¿”å›å“åº”ï¼‰ | ~300 |
| `scraper_v1/models.py` | æ‰€æœ‰ Pydantic è¯·æ±‚/å“åº”æ¨¡å‹ | ~120 |
| `scraper_v1/download_service.py` | ä¸‹è½½ç¼–æ’ã€ä»»åŠ¡çŠ¶æ€æœºã€é‡è¯•è°ƒåº¦ | ~350 |
| `scraper_v1/helpers.py` | å·¥å…·å‡½æ•°ï¼ˆ`_safe_name`, `_normalize_catalog_path` ç­‰ï¼‰ | ~80 |

### 3.2 ä¿®å¤åŒå†™ä¸€è‡´æ€§

**æ”¹ä¸ºä»¥ SQLite ä¸ºå•ä¸€å†™å…¥æºï¼Œå†…å­˜ä»…ä½œè¯»ç¼“å­˜**ï¼š

```mermaid
sequenceDiagram
    participant Route as è·¯ç”±å±‚
    participant Service as DownloadService
    participant DB as SQLite (å•ä¸€æ•°æ®æº)
    participant Cache as å†…å­˜ç¼“å­˜ (è¯»)

    Route->>Service: æäº¤ä¸‹è½½ä»»åŠ¡
    Service->>DB: create_task() [å†™å…¥]
    Service->>Cache: åŒæ­¥æ›´æ–°ç¼“å­˜
    Note over DB,Cache: ä»»ä½•å†™æ“ä½œéƒ½å…ˆ DB å Cache

    Route->>Service: æŸ¥è¯¢ä»»åŠ¡çŠ¶æ€
    Service->>Cache: å°è¯•å‘½ä¸­ç¼“å­˜
    alt ç¼“å­˜æœªå‘½ä¸­
        Service->>DB: get_task() [å›é€€æŸ¥è¯¢]
        DB-->>Cache: å›å¡«ç¼“å­˜
    end
    Cache-->>Route: è¿”å›çŠ¶æ€
```

### 3.3 ä¸‹è½½ä»»åŠ¡å¢åŠ æ–­ç‚¹ç»­ä¼  + å®æ—¶è¿›åº¦

```python
# download_service.py ä¸­çš„ worker
async def worker(index: int, image_url: str):
    output_path = output_dir / f"{index:03d}{ext}"
    if output_path.exists() and output_path.stat().st_size > 0:
        return  # æ–­ç‚¹ç»­ä¼ ï¼šè·³è¿‡å·²ä¸‹è½½

    result = await http_client.download_binary(...)

    # å®æ—¶è¿›åº¦ä¸ŠæŠ¥
    await self._update_progress(task_id, completed=current, total=total)
```

### 3.4 Task å¼•ç”¨ç®¡ç†

```python
# download_service.py
class DownloadService:
    _active_tasks: set[asyncio.Task] = set()

    def submit(self, coro):
        task = asyncio.create_task(coro)
        self._active_tasks.add(task)
        task.add_done_callback(self._active_tasks.discard)

    async def shutdown(self):
        for task in self._active_tasks:
            task.cancel()
        await asyncio.gather(*self._active_tasks, return_exceptions=True)
```

---

## Phase 4: å‰ç«¯ Store æ‹†åˆ†ï¼ˆ4hï¼‰

å°† `scraper.js`ï¼ˆ1319Lï¼‰æ‹†ä¸ºï¼š

| æ–°æ–‡ä»¶ | èŒè´£ | é¢„ä¼°è¡Œæ•° |
|--------|------|:--------:|
| `composables/useScraperApi.js` | API è°ƒç”¨å±‚ + é”™è¯¯æ˜ å°„ | ~200 |
| `composables/useDownloadQueue.js` | ä¸‹è½½é˜Ÿåˆ—è°ƒåº¦ + è½®è¯¢ | ~250 |
| `composables/useScraperConfig.js` | ç«™ç‚¹/æ¨¡å¼/UA é…ç½®ç®¡ç† | ~200 |
| `stores/scraper.js` | çŠ¶æ€èšåˆ + UI è¾…åŠ©è®¡ç®— | ~300 |

---

## æ–‡ä»¶å˜åŠ¨æ€»è§ˆ

```mermaid
graph LR
    subgraph æ–°å¢æ–‡ä»¶
        A1["scraper_v1/http_client.py"]
        A2["scraper_v1/cf_solver.py"]
        A3["scraper_v1/base.py"]
        A4["scraper_v1/models.py"]
        A5["scraper_v1/download_service.py"]
        A6["scraper_v1/helpers.py"]
        A7["composables/useScraperApi.js"]
        A8["composables/useDownloadQueue.js"]
        A9["composables/useScraperConfig.js"]
    end

    subgraph é‡åº¦ä¿®æ”¹
        B1["v1_scraper.py<br>1260L â†’ ~300L"]
        B2["mangaforfree.py<br>åˆ é™¤ç½‘ç»œå‡½æ•°"]
        B3["toongod.py<br>åˆ é™¤ç§æœ‰å¯¼å…¥"]
        B4["generic.py<br>åˆ é™¤ç½‘ç»œå‡½æ•°"]
        B5["providers.py<br>ProviderContext"]
        B6["state.py<br>+CookieStore"]
        B7["stores/scraper.js<br>1319L â†’ ~300L"]
    end

    subgraph è½»ä¿®æ”¹
        C1["task_store.py â€” æ–°å¢ progress å­—æ®µ"]
        C2["alerts.py â€” é›†æˆ CF å¤±è´¥å‘Šè­¦"]
        C3["__init__.py â€” æ›´æ–°å¯¼å‡º"]
    end

    subgraph æ–°å¢ä¾èµ–
        D1["requirements: +curl_cffi"]
        D2["docker-compose: +flaresolverr (å¯é€‰)"]
    end
```

---

## å®æ–½é¡ºåºä¸ä¾èµ–

```mermaid
gantt
    title çˆ¬è™« v2 é‡æ„å®æ–½è®¡åˆ’ï¼ˆçº¦ 2 å‘¨ï¼‰
    dateFormat  YYYY-MM-DD
    axisFormat  %m-%d

    section Phase 1 - ç½‘ç»œå±‚
    http_client.py + curl_cffi    :p1a, 2026-02-17, 3d
    state.py CookieStore          :p1c, 2026-02-17, 1d
    cf_solver.py é˜¶æ¢¯å¯¹æŠ—         :p1b, after p1a, 2d

    section Phase 2 - Provider
    base.py å…±äº«é€»è¾‘æŠ½å–           :p2a, after p1a, 1d
    ProviderContext é‡æ„           :p2b, after p2a, 1d

    section Phase 3 - ä¸šåŠ¡å±‚
    models.py æ‹†åˆ†                :p3a, after p1b, 0.5d
    download_service.py           :p3b, after p3a, 2d
    v1_scraper.py ç˜¦èº«            :p3c, after p3b, 1d
    åŒå†™ä¸€è‡´æ€§ä¿®å¤                 :p3d, after p3c, 1d

    section Phase 4 - å‰ç«¯
    Store æ‹†åˆ†ä¸º composables       :p4a, after p2b, 2d
```

---

## éªŒè¯è®¡åˆ’

### è‡ªåŠ¨åŒ–æµ‹è¯•

```bash
# ç°æœ‰åç«¯æµ‹è¯•ï¼ˆå¿…é¡»å…¨éƒ¨é€šè¿‡ï¼‰
pytest tests/test_v1_scraper_phase2.py tests/test_v1_scraper_phase3.py tests/test_v1_scraper_phase4.py -v

# ç°æœ‰å‰ç«¯æµ‹è¯•ï¼ˆå¿…é¡»å…¨éƒ¨é€šè¿‡ï¼‰
cd frontend && npm test
```

### å†’çƒŸæµ‹è¯•

| æµ‹è¯•é¡¹ | æ–¹æ³• | è¦†ç›–ç‚¹ |
|--------|------|--------|
| ToonGod æœç´¢ | å‰ç«¯è¾“å…¥å…³é”®è¯æœç´¢ | HttpClient + curl_cffi + Provider |
| ç« èŠ‚ä¸‹è½½ | é€‰æ‹©ä¸€ç« ä¸‹è½½ | ä¸‹è½½æœåŠ¡ + æ–­ç‚¹ç»­ä¼  + è¿›åº¦ä¸ŠæŠ¥ |
| Cookie æ³¨å…¥ | ä¸Šä¼ çŠ¶æ€æ–‡ä»¶åé‡æ–°æœç´¢ | CookieStore + state.py |
| CF 403 å›é€€ | Mock ä¸€ä¸ª 503 å“åº” | cf_solver é˜¶æ¢¯é€»è¾‘ |
| ä»»åŠ¡æŸ¥è¯¢ | é‡å¯æœåŠ¡åæŸ¥è¯¢å†å²ä»»åŠ¡ | SQLite å•ä¸€æ•°æ®æº |
| å‰ç«¯ Store | å„ composable åŠŸèƒ½æ­£å¸¸ | Store æ‹†åˆ†ä¸ä¸¢åŠŸèƒ½ |

---

## é£é™©ä¸ç¼“è§£

| é£é™© | çº§åˆ« | ç¼“è§£æªæ–½ |
|------|:----:|----------|
| `curl_cffi` åœ¨ Docker alpine ä¸Šç¼–è¯‘å¤±è´¥ | ğŸŸ¡ ä¸­ | ä½¿ç”¨ `python:3.12-slim`ï¼ˆå·²æœ‰ glibcï¼‰ï¼Œpip æä¾›é¢„ç¼–è¯‘ wheel |
| FlareSolverr å†…å­˜å¼€é”€å¤§ | ğŸŸ¢ ä½ | è®¾ä¸ºå¯é€‰ sidecarï¼Œä¸åµŒå…¥ä¸»å®¹å™¨ |
| è·¯ç”±æ‹†åˆ†å¼•å…¥å›å½’ | ğŸŸ¡ ä¸­ | æ¯ä¸ª Phase å®Œæˆåè·‘å…¨é‡æµ‹è¯• |
| å‰ç«¯ store æ‹†åˆ†é—æ¼å“åº”å¼å¼•ç”¨ | ğŸŸ¡ ä¸­ | `npm test` + æ‰‹åŠ¨å†’çƒŸæµ‹è¯• |
| Provider å‡½æ•°ç­¾åå˜æ›´å½±å“æµ‹è¯• | ğŸŸ¢ ä½ | æµ‹è¯•ä½¿ç”¨ mockï¼Œç­¾åå˜æ›´ä¸å½±å“ |

---

## è¡¥å……ï¼šåŸæ–¹æ¡ˆé—æ¼äº‹é¡¹

### è¡¥å…… 1ï¼šAdmin è·¯ç”±ä¸ Scraper è·¯ç”±çš„å…³ç³»

Scraper çš„ Admin çº§ç«¯ç‚¹ï¼ˆtasks/metrics/health/alerts/queueï¼‰**å·²ç»ç‹¬ç«‹å­˜åœ¨äº `admin.py`**ï¼ˆ`admin.py:469-595`ï¼‰ï¼Œä½¿ç”¨ `/admin/scraper/*` å‰ç¼€ï¼Œå¹¶éåœ¨ `v1_scraper.py` ä¸­ã€‚å‰ç«¯ç”± `adminScraper.js`ï¼ˆ249Lï¼‰é©±åŠ¨ã€‚

å½“å‰çŠ¶æ€ï¼š

| ç«¯ç‚¹ | æ‰€åœ¨æ–‡ä»¶ |
|------|----------|
| `/api/v1/scraper/*`ï¼ˆsearch/catalog/chapters/download/taskï¼‰ | `routes/v1_scraper.py` |
| `/admin/scraper/*`ï¼ˆtasks/metrics/health/alerts/queueï¼‰ | `routes/admin.py` |

Phase 3 æ‹†åˆ† `v1_scraper.py` æ—¶ï¼ŒAdmin è·¯ç”±**æ— éœ€é¢å¤–å¤„ç†**ï¼ˆå·²ç»åˆ†ç¦»ï¼‰ã€‚ä½†éœ€æ³¨æ„ä¸¤ä¸ªè·¯ç”±æ–‡ä»¶å…±äº« `_get_task_store()` ç­‰å†…éƒ¨ä¾èµ–ï¼Œé‡æ„ååº”æ”¹ä¸ºä» `download_service.py` ç»Ÿä¸€æš´éœ²æ¥å£ã€‚

> [!WARNING]
> ç°æœ‰ `/admin/scraper/*` è·¯ç”±å¥‘çº¦å·²è¢«å‰ç«¯ `adminScraper.js` ä½¿ç”¨ï¼Œ**ä¸å¯å˜æ›´ prefix**ã€‚

---

### è¡¥å…… 2ï¼šParser æ¨¡å—ä¸ HttpClient ç»Ÿä¸€

`v1_parser.py`ï¼ˆ249Lï¼‰æ˜¯ä¸çˆ¬è™«ç´§å¯†å…³è”çš„ URL è§£ææ¨¡å—ï¼Œå‰ç«¯ `scraper.js` çš„ `parseUrl()` ç›´æ¥è°ƒç”¨å®ƒã€‚ä½†å®ƒ**ç‹¬ç«‹ä½¿ç”¨ `httpx` åŒæ­¥å®¢æˆ·ç«¯ï¼ˆåœ¨çº¿ç¨‹æ± ä¸­æ‰§è¡Œï¼‰åš HTTP è¯·æ±‚**ï¼Œä¸èµ° `aiohttp`ï¼Œä¹Ÿä¸èµ°æœªæ¥çš„ `HttpClient`ï¼š

```python
# v1_parser.py:34,48 â€” ç”¨çš„æ˜¯ httpx.Client + asyncio.to_thread
def _fetch_html_sync(url, ...):
    with httpx.Client(...) as client:
        resp = client.get(url, ...)

async def _fetch_html(url, ...):
    return await asyncio.to_thread(_fetch_html_sync, url, ...)
```

**å»ºè®®**ï¼šPhase 1 ç»Ÿä¸€ HttpClient æ—¶ï¼Œ`v1_parser.py` ä¹Ÿåº”æ”¹ä¸ºè°ƒç”¨ `ScraperHttpClient.fetch_html()`ï¼Œå…±äº« TLS ä¼ªè£…ã€Cookie æ³¨å…¥å’Œé™æµèƒ½åŠ›ã€‚å¦åˆ™ Parser ä»ç„¶ç”¨è£¸ httpxï¼Œä¼šæˆä¸º CF æ‹¦æˆªçš„è½¯è‚‹ã€‚

---

### è¡¥å…… 3ï¼šå°é¢å›¾ç‰‡ä»£ç†ç«¯ç‚¹

`/api/v1/scraper/image` æ˜¯ä¸€ä¸ªé‡è¦çš„ä»£ç†ç«¯ç‚¹ï¼Œå‰ç«¯ç”¨å®ƒæ¥ä»£ç†åŠ è½½æ¼«ç”»å°é¢ï¼ˆç»•è¿‡ CORS å’Œé˜²ç›—é“¾ï¼‰ï¼š

```javascript
// scraper.js:638
return `/api/v1/scraper/image?${params.toString()}`
```

è¯¥ç«¯ç‚¹å†…éƒ¨ä¹Ÿè‡ªå»º `aiohttp.ClientSession`ï¼ˆ`v1_scraper.py:1249`ï¼‰ã€‚Phase 1 ç»Ÿä¸€ HttpClient æ—¶éœ€ç‰¹åˆ«æ³¨æ„ï¼š
- å›¾ç‰‡ä»£ç†éœ€æ”¯æŒ**æµå¼å“åº”**ï¼ˆä¸èƒ½å…ˆä¸‹è½½åˆ°å†…å­˜å†è¿”å›ï¼‰
- éœ€ä¿ç•™å¯¹ `provider_allows_image_host()` çš„å®‰å…¨æ£€æŸ¥ï¼ˆé˜²æ­¢è¢«æ»¥ç”¨ä¸ºå¼€æ”¾ä»£ç†ï¼‰
- `HttpClient.download_binary()` è®¾è®¡æ—¶éœ€æ”¯æŒ"æµå¼ä»£ç†"æ¨¡å¼ï¼Œè€Œéä»…"ä¸‹è½½åˆ°æ–‡ä»¶"

---

### è¡¥å…… 4ï¼šä¸ Qt åˆ†ç¦»è®¡åˆ’çš„æ‰§è¡Œé¡ºåº

å½“å‰æ­£åœ¨ worktree `codex/qt-separation-20260215` ä¸­å¹¶è¡Œå®æ–½ [Qt åˆ†ç¦»æ–¹æ¡ˆ](file:///Users/xa/Desktop/projiect/manga-translator-ui_å‰¯æœ¬/docs/refactoring/qt-separation-plan.md)ã€‚ä¸¤ä¸ªé‡æ„åœ¨ä»¥ä¸‹æ–‡ä»¶å­˜åœ¨å¹¶è¡Œæ”¹åŠ¨å†²çªï¼š

| å†²çªæ–‡ä»¶ | Qt åˆ†ç¦»æ”¹åŠ¨ | Scraper v2 æ”¹åŠ¨ |
|----------|-----------|----------------|
| `manga_translator/server/main.py` | locales è·¯å¾„è¿ç§»ã€æ–°å¢ `locales_router` | `download_service` åˆå§‹åŒ–ã€è·¯ç”±ç˜¦èº«åçš„å¯¼å…¥å˜æ›´ |
| `manga_translator/server/routes/__init__.py` | æ–°å¢ `locales_router` å¯¼å‡º | å¯¼å‡ºåˆ—è¡¨éšè·¯ç”±ç˜¦èº«å¯èƒ½å˜æ›´ |

> [!NOTE]
> Admin è·¯ç”±å·²ç‹¬ç«‹åœ¨ `admin.py` ä¸­ï¼ˆè§è¡¥å…… 1ï¼‰ï¼Œæ­¤å¤„ä¸æ¶‰åŠ admin è·¯ç”±æ‹†åˆ†åŠ¨ä½œã€‚

**å»ºè®®æ‰§è¡Œé¡ºåº**ï¼š
1. **å…ˆå®Œæˆ Qt åˆ†ç¦»**ï¼ˆå·²åœ¨è¿›è¡Œä¸­ï¼‰ï¼Œåˆå…¥ main
2. **å†å¯åŠ¨ Scraper v2**ï¼ŒåŸºäºåˆå…¥åçš„ä»£ç å¼€å§‹

---

### è¡¥å…… 5ï¼štask_store.py çš„ DB Migration

æ–°å¢"å®æ—¶è¿›åº¦"å­—æ®µï¼ˆ`progress_completed`ã€`progress_total`ï¼‰éœ€è¦ SQLite schema å˜æ›´ã€‚`ScraperTaskStore` å·²æœ‰ `_ensure_migrations()` æœºåˆ¶ï¼ˆ`task_store.py:126`ï¼‰ï¼Œä¼šè‡ªåŠ¨ `ALTER TABLE ADD COLUMN`ï¼š

```python
_EXTRA_COLUMNS: dict[str, str] = {
    "retry_count": "INTEGER NOT NULL DEFAULT 0",
    "max_retries": "INTEGER NOT NULL DEFAULT 2",
    # ... ç°æœ‰è¿ç§»
}
```

Phase 3 æ–°å¢è¿›åº¦å­—æ®µæ—¶ï¼Œåªéœ€åœ¨ `_EXTRA_COLUMNS` ä¸­è¿½åŠ ï¼š

```python
"progress_completed": "INTEGER NOT NULL DEFAULT 0",
"progress_total": "INTEGER NOT NULL DEFAULT 0",
```

ç°æœ‰çš„è‡ªåŠ¨è¿ç§»æœºåˆ¶ä¼šåœ¨å¯åŠ¨æ—¶æ£€æµ‹å¹¶æ·»åŠ ç¼ºå¤±åˆ—ï¼Œ**æ— éœ€æ‰‹åŠ¨è¿ç§»è„šæœ¬**ã€‚ä½†éœ€éªŒè¯ç”Ÿäº§ç¯å¢ƒçš„ `scraper_tasks.db` èƒ½å¦æ­£å¸¸è¿ç§»ã€‚

---

### è¡¥å…… 6ï¼šrate_limit_rps ä¸ concurrency æ¦‚å¿µåŒºåˆ†

å½“å‰ä»£ç ä¸­ `concurrency`ï¼ˆå¹¶å‘æ•°ï¼‰ å’Œ `rate_limit_rps`ï¼ˆæ¯ç§’è¯·æ±‚æ•°ï¼‰æ˜¯ä¸¤ä¸ªç‹¬ç«‹çš„å‰ç«¯é…ç½®ï¼Œä½†åç«¯åªå®ç°äº† concurrencyï¼ˆé€šè¿‡ `asyncio.Semaphore`ï¼‰ï¼Œ**`rate_limit_rps` å®é™…ä¸Šæ²¡æœ‰ç”Ÿæ•ˆ**ï¼š

```python
# v1_scraper.py:745 â€” åªç”¨äº† concurrencyï¼Œrate_limit_rps æœªä½¿ç”¨
connector = aiohttp.TCPConnector(limit=max(1, min(32, int(req.concurrency or 6))))
```

ç»Ÿä¸€ HttpClient æ—¶éœ€åŒºåˆ†æ¸…æ¥šï¼š

| æ¦‚å¿µ | å«ä¹‰ | å®ç°æ–¹å¼ |
|------|------|---------|
| **concurrency** | åŒä¸€æ—¶åˆ»æœ€å¤§å¹¶å‘è¿æ¥æ•° | `asyncio.Semaphore(N)` |
| **rate_limit_rps** | æ¯ç§’æœ€å¤§è¯·æ±‚æ¬¡æ•° | ä»¤ç‰Œæ¡¶æˆ–æ»‘åŠ¨çª—å£ï¼ˆéœ€æ–°å¢å®ç°ï¼‰ |

ä¸¤è€…ç¼ºä¸€ä¸å¯ï¼šä»…æœ‰ concurrency é™åˆ¶å¹¶å‘æ•°ä½†ä¸é™é€Ÿç‡ï¼Œ6 ä¸ªå¹¶å‘è¯·æ±‚å¯èƒ½åœ¨ 100ms å†…å…¨éƒ¨å‘å‡ºï¼Œè§¦å‘ç›®æ ‡ç«™ç‚¹çš„é¢‘ç‡å°ç¦ã€‚
